{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YHDgeG5r1BVY",
      "metadata": {
        "id": "YHDgeG5r1BVY"
      },
      "source": [
        "# Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf51cdf-39fc-4267-b9ed-f9899f398ac5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbf51cdf-39fc-4267-b9ed-f9899f398ac5",
        "outputId": "5591e2ce-4237-48a3-ab18-f5fed6a8361c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/nightly/torch-2.5.1+cu124.html\n",
            "Collecting pyg-lib\n",
            "  Downloading https://data.pyg.org/whl/nightly/torch-2.5.0%2Bcu124/pyg_lib-0.4.0.dev20250208%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyg-lib\n",
            "Successfully installed pyg-lib-0.4.0.dev20250208+pt25cu124\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-sjeu6oc3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-sjeu6oc3\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit bb6601c8666e69205a7a4d1d981b771e9bae6880\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2025.1.31)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1173586 sha256=3dc81b9d51534ac1270c7cf278a790aec857b404cd8c8d014a5849e6dcb042d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1kvx0y2_/wheels/93/bb/85/bfec4ee59b2563f74ec87cc2c91c6a4d3e40d3dcdec8ee5afe\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Install required packages.\n",
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1BCsNFt0selb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BCsNFt0selb",
        "outputId": "897f6cef-6585-4736-d2af-4f2338b4ddeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.4.tar.gz (91 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/91.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (2.5.1+cu124)\n",
            "Collecting ninja (from mamba-ssm)\n",
            "  Using cached ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (4.48.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mamba-ssm) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2025.1.31)\n",
            "Using cached ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.4-cp311-cp311-linux_x86_64.whl size=323672993 sha256=8a0be01153fa30727a9e69024fbe061eb92c7ba4416d2049c5fc3107ed91d852\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/5e/64/cfcb5dfe4f854944456e031c34953dc872af1ad7c206145d4a\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mamba-ssm\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed mamba-ssm-2.2.4 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install mamba-ssm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bN6xXMPszLxa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN6xXMPszLxa",
        "outputId": "02001c51-8d3e-4cd0-944a-998e63e9766a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton) (3.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cfb810b-2979-432a-9186-3a03f6ae086c",
      "metadata": {
        "id": "4cfb810b-2979-432a-9186-3a03f6ae086c"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch\n",
        "from torch.nn import (\n",
        "    BatchNorm1d,\n",
        "    Embedding,\n",
        "    Linear,\n",
        "    ModuleList,\n",
        "    LeakyReLU,\n",
        "    Sequential,\n",
        "    Sigmoid\n",
        ")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "import inspect\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, Linear, Sequential\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import reset\n",
        "from torch_geometric.nn.resolver import (\n",
        "    activation_resolver,\n",
        "    normalization_resolver,\n",
        ")\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "from torch_geometric.loader import DataLoader, LinkNeighborLoader\n",
        "from mamba_ssm import Mamba, Mamba2\n",
        "from torch_geometric.utils import degree, sort_edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49609a76-1d75-438c-b9f0-e88439dd39d1",
      "metadata": {
        "id": "49609a76-1d75-438c-b9f0-e88439dd39d1"
      },
      "outputs": [],
      "source": [
        "def permute_within_batch(x, batch):\n",
        "    # Enumerate over unique batch indices\n",
        "    unique_batches = torch.unique(batch)\n",
        "\n",
        "    # Initialize list to store permuted indices\n",
        "    permuted_indices = []\n",
        "\n",
        "    for batch_index in unique_batches:\n",
        "        # Extract indices for the current batch\n",
        "        indices_in_batch = (batch == batch_index).nonzero().squeeze()\n",
        "\n",
        "        # Permute indices within the current batch\n",
        "        permuted_indices_in_batch = indices_in_batch[torch.randperm(len(indices_in_batch))]\n",
        "\n",
        "        # Append permuted indices to the list\n",
        "        permuted_indices.append(permuted_indices_in_batch)\n",
        "\n",
        "    # Concatenate permuted indices into a single tensor\n",
        "    permuted_indices = torch.cat(permuted_indices)\n",
        "\n",
        "    return permuted_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H8U7HwNQz4W5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8U7HwNQz4W5",
        "outputId": "bed66eb2-2140-40de-8ac3-a4671a3ead5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gxpSjtr4zpfw",
      "metadata": {
        "id": "gxpSjtr4zpfw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def str_list_to_float(str_list):\n",
        "    return [float(item) for item in str_list]\n",
        "\n",
        "\n",
        "def str_list_to_int(str_list):\n",
        "    return [int(item) for item in str_list]\n",
        "\n",
        "\n",
        "def read_embeddings(filename):\n",
        "    \"\"\"read pretrained node embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        n_node, n_embed = str_list_to_int(f.readline().split())\n",
        "        lines = f.readlines()[1:]  # skip the first line\n",
        "        embedding_matrix = np.random.rand(n_node, n_embed)\n",
        "        for line in lines:\n",
        "            emd = line.split()\n",
        "            embedding_matrix[int(emd[0]), :] = str_list_to_float(emd[1:])\n",
        "        return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def read_edges_from_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    if not lines:  # Check if file is empty\n",
        "        raise ValueError(f\"Edge file {filename} is empty!\")\n",
        "\n",
        "    edges = [list(map(int, line.split())) for line in lines]  # Convert to int list\n",
        "    edges = torch.tensor(edges, dtype=torch.long)  # Ensure proper tensor type\n",
        "\n",
        "    if edges.shape[1] != 2:  # Ensure it's a 2-column tensor\n",
        "        raise ValueError(f\"Edge file {filename} has incorrect shape: {edges.shape}\")\n",
        "\n",
        "    return edges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LvJKNNf-zkLd",
      "metadata": {
        "id": "LvJKNNf-zkLd"
      },
      "outputs": [],
      "source": [
        "train_edges_filename = \"/content/drive/MyDrive/Emb_Data/Data/Biogrid-human/bio-grid-human_train.txt\"\n",
        "test_edges_filename = \"/content/drive/MyDrive/Emb_Data/Data/Biogrid-human/bio-grid-human_test.txt\"\n",
        "emb_filename = \"/content/drive/MyDrive/Emb_Data/Emb/biogrid-human/DeepWalk/fuzzy_emb.txt\"\n",
        "n_nodes = 5242\n",
        "n_embed = 50\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2Mu7XYJbGNR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d2Mu7XYJbGNR",
        "outputId": "8f269de5-001d-45af-dcd0-c6869a846ae1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1131a620-e047-4191-a843-f7f285877e89",
      "metadata": {
        "id": "1131a620-e047-4191-a843-f7f285877e89"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "train_edges = read_edges_from_file(train_edges_filename).to(device).contiguous().T\n",
        "test_edges = read_edges_from_file(test_edges_filename).to(device).contiguous().T\n",
        "train_edges_label = torch.ones(train_edges.size(1), device=device, dtype=torch.long)\n",
        "test_edges_label = torch.ones(test_edges.size(1), device=device, dtype=torch.long)\n",
        "node_embeddings = read_embeddings(emb_filename).to(device)\n",
        "\n",
        "train_data = Data(x=node_embeddings, edge_index=train_edges.contiguous())\n",
        "test_data = Data(x=node_embeddings, edge_index=test_edges.contiguous())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5MFOd1fvu4eR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MFOd1fvu4eR",
        "outputId": "2539e20d-ac1f-41d9-e038-0ff01ae0ce4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_edges shape: torch.Size([2, 56127])\n",
            "train_edges min: 0 max: 9435\n",
            "train_edges dtype: torch.int64\n"
          ]
        }
      ],
      "source": [
        "print(\"train_edges shape:\", train_edges.shape)\n",
        "print(\"train_edges min:\", train_edges.min().item(), \"max:\", train_edges.max().item())\n",
        "print(\"train_edges dtype:\", train_edges.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5-AqU2OCm6Kc",
      "metadata": {
        "id": "5-AqU2OCm6Kc"
      },
      "outputs": [],
      "source": [
        "train_loader = LinkNeighborLoader(data = train_data,\n",
        "                          num_neighbors = [20, 20],\n",
        "                          batch_size=64,\n",
        "                          neg_sampling='binary',\n",
        "                          shuffle=True,\n",
        "                          edge_label_index = train_data.edge_index.contiguous(),\n",
        "                          edge_label=torch.ones(train_data.edge_index.size(1)))\n",
        "test_loader = LinkNeighborLoader(data = test_data,\n",
        "                          num_neighbors = [20, 20],\n",
        "                          batch_size=64,\n",
        "                          neg_sampling = 'binary',\n",
        "                          shuffle=False,\n",
        "                          edge_label_index = test_data.edge_index.contiguous(),\n",
        "                          edge_label=torch.ones(test_data.edge_index.size(1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dc6712-8c4a-44f7-94e9-d582c5e16bcd",
      "metadata": {
        "id": "34dc6712-8c4a-44f7-94e9-d582c5e16bcd"
      },
      "outputs": [],
      "source": [
        "class GPSConv(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        conv: Optional[MessagePassing],\n",
        "        heads: int = 1,\n",
        "        dropout: float = 0.0,\n",
        "        attn_dropout: float = 0.0,\n",
        "        act: str = 'relu',\n",
        "        att_type: str = 'transformer',\n",
        "        order_by_degree: bool = False,\n",
        "        shuffle_ind: int = 0,\n",
        "        d_state: int = 16,\n",
        "        d_conv: int = 4,\n",
        "        act_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        norm: Optional[str] = 'batch_norm',\n",
        "        norm_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.conv = conv\n",
        "        self.heads = heads\n",
        "        self.dropout = dropout\n",
        "        self.att_type = att_type\n",
        "        self.shuffle_ind = shuffle_ind\n",
        "        self.order_by_degree = order_by_degree\n",
        "\n",
        "        assert (self.order_by_degree==True and self.shuffle_ind==0) or (self.order_by_degree==False), f'order_by_degree={self.order_by_degree} and shuffle_ind={self.shuffle_ind}'\n",
        "\n",
        "        if self.att_type == 'transformer':\n",
        "            self.attn = torch.nn.MultiheadAttention(\n",
        "                channels,\n",
        "                heads,\n",
        "                dropout=attn_dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        if self.att_type == 'mamba':\n",
        "            self.self_attn = Mamba(\n",
        "                d_model=channels,\n",
        "                d_state=d_state,\n",
        "                d_conv=d_conv,\n",
        "                expand=1,\n",
        "            )\n",
        "\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels * 2),\n",
        "            activation_resolver(act, **(act_kwargs or {})),\n",
        "            Dropout(dropout),\n",
        "            Linear(channels * 2, channels),\n",
        "            Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        norm_kwargs = norm_kwargs or {}\n",
        "        self.norm1 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "        self.norm2 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "        self.norm3 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "\n",
        "        self.norm_with_batch = False\n",
        "        if self.norm1 is not None:\n",
        "            signature = inspect.signature(self.norm1.forward)\n",
        "            self.norm_with_batch = 'batch' in signature.parameters\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        if self.conv is not None:\n",
        "            self.conv.reset_parameters()\n",
        "        self.attn._reset_parameters()\n",
        "        reset(self.mlp)\n",
        "        if self.norm1 is not None:\n",
        "            self.norm1.reset_parameters()\n",
        "        if self.norm2 is not None:\n",
        "            self.norm2.reset_parameters()\n",
        "        if self.norm3 is not None:\n",
        "            self.norm3.reset_parameters()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        edge_index: Adj,\n",
        "        batch: Optional[torch.Tensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> Tensor:\n",
        "        r\"\"\"Runs the forward pass of the module.\"\"\"\n",
        "        hs = []\n",
        "        if self.conv is not None:  # Local MPNN.\n",
        "            h = self.conv(x, edge_index, **kwargs)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            h = h + x\n",
        "            if self.norm1 is not None:\n",
        "                if self.norm_with_batch:\n",
        "                    h = self.norm1(h, batch)\n",
        "                else:\n",
        "                    h = self.norm1(h)\n",
        "            hs.append(h)\n",
        "\n",
        "        ### Global attention transformer-style model.\n",
        "        if self.att_type == 'transformer':\n",
        "            h, mask = to_dense_batch(x, batch)\n",
        "            h, _ = self.attn(h, h, h, key_padding_mask=~mask, need_weights=False)\n",
        "            h = h[mask]\n",
        "\n",
        "        if self.att_type == 'mamba':\n",
        "\n",
        "            if self.order_by_degree:\n",
        "                deg = degree(edge_index[0], x.shape[0]).to(torch.long)\n",
        "                print(f\"shape of deg : {deg.shape}\")\n",
        "                order_tensor = torch.stack([batch, deg], 1).T\n",
        "                _, x = sort_edge_index(order_tensor)\n",
        "\n",
        "            if self.shuffle_ind == 0:\n",
        "                h, mask = to_dense_batch(x)\n",
        "                h = self.self_attn(h)[mask]\n",
        "            else:\n",
        "                mamba_arr = []\n",
        "                for _ in range(self.shuffle_ind):\n",
        "                    h_ind_perm = permute_within_batch(x, batch)\n",
        "                    h_i, mask = to_dense_batch(x[h_ind_perm], batch)\n",
        "                    h_i = self.self_attn(h_i)[mask][h_ind_perm]\n",
        "                    mamba_arr.append(h_i)\n",
        "                h = sum(mamba_arr) / self.shuffle_ind\n",
        "        ###\n",
        "\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = h + x  # Residual connection.\n",
        "        if self.norm2 is not None:\n",
        "            if self.norm_with_batch:\n",
        "                h = self.norm2(h, batch=batch)\n",
        "            else:\n",
        "                h = self.norm2(h)\n",
        "        hs.append(h)\n",
        "\n",
        "        out = sum(hs)  # Combine local and global outputs.\n",
        "\n",
        "        out = out + self.mlp(out)\n",
        "        if self.norm3 is not None:\n",
        "            if self.norm_with_batch:\n",
        "                out = self.norm3(out, batch=batch)\n",
        "            else:\n",
        "                out = self.norm3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.channels}, '\n",
        "                f'conv={self.conv}, heads={self.heads})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9fea5c7-9fc9-4388-98fd-8c45787d0abc",
      "metadata": {
        "id": "d9fea5c7-9fc9-4388-98fd-8c45787d0abc"
      },
      "outputs": [],
      "source": [
        "class GraphModel(torch.nn.Module):\n",
        "    def __init__(self, channels: int, pe_dim: int, num_layers: int, model_type: str, shuffle_ind: int, d_state: int, d_conv: int, order_by_degree: False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_emb = Embedding(28, channels - pe_dim)\n",
        "        self.pe_lin = Linear(20, pe_dim)\n",
        "        self.pe_norm = BatchNorm1d(20)\n",
        "        self.edge_emb = Embedding(4, channels)\n",
        "        self.model_type = model_type #specifies which architecutre we are using\n",
        "        self.shuffle_ind = shuffle_ind\n",
        "        self.order_by_degree = order_by_degree\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.sigmoid = Sigmoid()\n",
        "        self.convs = ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            nn = Sequential(\n",
        "                Linear(channels, channels),\n",
        "                LeakyReLU(0.01),\n",
        "                Linear(channels, channels),\n",
        "            )\n",
        "            if self.model_type == 'gine':\n",
        "                conv = GINConv(nn) #node features and edge indices needed\n",
        "\n",
        "            if self.model_type == 'mamba':\n",
        "                conv = GPSConv(channels, GINConv(nn), heads=4, attn_dropout=0.5,\n",
        "                               att_type='mamba',\n",
        "                               shuffle_ind=self.shuffle_ind,\n",
        "                               order_by_degree=self.order_by_degree,\n",
        "                               d_state=d_state, d_conv=d_conv)\n",
        "\n",
        "            if self.model_type == 'transformer':\n",
        "                conv = GPSConv(channels, GINConv(nn), heads=4, attn_dropout=0.5, att_type='transformer')\n",
        "\n",
        "            # conv = GINEConv(nn)\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        self.mlp = Sequential(\n",
        "            self.dropout,\n",
        "            Linear(channels, channels // 2),\n",
        "            LeakyReLU(0.01),\n",
        "            Linear(channels // 2, channels // 4),\n",
        "            LeakyReLU(0.01),\n",
        "            self.dropout,\n",
        "            Linear(channels // 4, 1),\n",
        "        )\n",
        "        self.linear = Linear(n_embed, channels)\n",
        "    def forward(self, x, edge_index, edge_label_index):\n",
        "        num_nodes = x.size(0)\n",
        "        #print(f\"Max edge index: {edge_index.max()}, Num nodes: {num_nodes}\")\n",
        "        assert edge_index.max() < num_nodes, \"Edge index out of bounds!\"\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        for conv in self.convs:\n",
        "            if self.model_type == 'gine':\n",
        "                x = conv(x, edge_index)\n",
        "            else:\n",
        "                x = conv(x, edge_index)\n",
        "\n",
        "        pred = self.mlp(x)\n",
        "        # Step 2: Get embeddings for the source and target nodes of the edges\n",
        "        edge_src = x[edge_label_index[0]]  # Embeddings of source nodes\n",
        "        edge_dst = x[edge_label_index[1]]  # Embeddings of target nodes\n",
        "\n",
        "        # Step 3: Compute edge scores (using dot product as the decoder)\n",
        "        # Use the dot product as the decoder for link prediction\n",
        "        edge_features = edge_src * edge_dst  # Element-wise multiplication\n",
        "        pred = self.mlp(edge_features)       # Pass through MLP for final prediction\n",
        "        pred = pred.squeeze(-1)              # Remove last dimension if it exists\n",
        "\n",
        "\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V8x7n3QwQwXv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8x7n3QwQwXv",
        "outputId": "4d5567d4-35a1-4d8b-fc2c-48ffc1d382c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 877/877 [00:11<00:00, 73.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Loss: 0.2207, Accuracy: 0.9137, F1: 0.9172, Precision: 0.8814, Recall: 0.9562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 200.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.2133, Accuracy: 0.9323, F1: 0.9279, Precision: 0.9920, Recall: 0.8716\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 877/877 [00:11<00:00, 73.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 0.1235, Accuracy: 0.9584, F1: 0.9591, Precision: 0.9427, Recall: 0.9762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 207.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.1057, Accuracy: 0.9690, F1: 0.9689, Precision: 0.9722, Recall: 0.9655\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 877/877 [00:12<00:00, 71.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 002, Loss: 0.1022, Accuracy: 0.9668, F1: 0.9673, Precision: 0.9543, Recall: 0.9807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 191.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.1106, Accuracy: 0.9648, F1: 0.9642, Precision: 0.9806, Recall: 0.9484\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 877/877 [00:12<00:00, 72.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 003, Loss: 0.0894, Accuracy: 0.9717, F1: 0.9721, Precision: 0.9602, Recall: 0.9842\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 141.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.0792, Accuracy: 0.9759, F1: 0.9757, Precision: 0.9843, Recall: 0.9671\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 877/877 [00:12<00:00, 73.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 004, Loss: 0.0817, Accuracy: 0.9738, F1: 0.9740, Precision: 0.9642, Recall: 0.9841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 197.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.0822, Accuracy: 0.9748, F1: 0.9752, Precision: 0.9621, Recall: 0.9886\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 877/877 [00:12<00:00, 70.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 005, Loss: 0.0740, Accuracy: 0.9763, F1: 0.9766, Precision: 0.9670, Recall: 0.9863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 196.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.0739, Accuracy: 0.9751, F1: 0.9756, Precision: 0.9592, Recall: 0.9925\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 877/877 [00:12<00:00, 70.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 006, Loss: 0.0710, Accuracy: 0.9773, F1: 0.9775, Precision: 0.9678, Recall: 0.9874\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 202.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.0619, Accuracy: 0.9805, F1: 0.9808, Precision: 0.9670, Recall: 0.9950\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 877/877 [00:12<00:00, 71.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 007, Loss: 0.0688, Accuracy: 0.9779, F1: 0.9781, Precision: 0.9691, Recall: 0.9873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 194.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.0716, Accuracy: 0.9776, F1: 0.9777, Precision: 0.9762, Recall: 0.9792\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 877/877 [00:12<00:00, 72.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 008, Loss: 0.0657, Accuracy: 0.9790, F1: 0.9791, Precision: 0.9712, Recall: 0.9872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 200.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.0868, Accuracy: 0.9702, F1: 0.9698, Precision: 0.9827, Recall: 0.9572\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 877/877 [00:12<00:00, 72.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 009, Loss: 0.0639, Accuracy: 0.9793, F1: 0.9795, Precision: 0.9715, Recall: 0.9876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 98/98 [00:00<00:00, 188.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation - Loss: 0.0718, Accuracy: 0.9769, F1: 0.9772, Precision: 0.9643, Recall: 0.9905\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = GraphModel(channels=64, pe_dim=4, num_layers=2,\n",
        "                   model_type='mamba',\n",
        "                   shuffle_ind=0, order_by_degree=False,\n",
        "                   d_conv=2, d_state=4,\n",
        "                  ).to(device)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if torch.isnan(param).any() or torch.isinf(param).any():\n",
        "        print(f\"NaN or Inf detected in parameter: {name}\")\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
        "                              min_lr=0.00001)\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "def evaluate(loader, model):\n",
        "    \"\"\"Evaluate the model and calculate metrics.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_examples = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sampled_data in tqdm(loader, desc=\"Evaluating\"):\n",
        "            # Move data to device\n",
        "            sampled_data = sampled_data.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred = model(sampled_data.x, sampled_data.edge_index, sampled_data.edge_label_index)\n",
        "            # Ground truth labels\n",
        "            ground_truth = sampled_data.edge_label\n",
        "\n",
        "            # Apply binary cross entropy with logits\n",
        "            loss = criterion(pred, ground_truth.float())\n",
        "            total_loss += float(loss) * pred.numel()\n",
        "            total_examples += pred.numel()\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            all_preds.append(torch.sigmoid(pred).cpu().numpy())\n",
        "            all_labels.append(ground_truth.cpu().numpy())\n",
        "\n",
        "    # Concatenate all predictions and labels\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Convert probabilities to binary predictions using threshold 0.5\n",
        "    binary_preds = (all_preds >= 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, binary_preds)\n",
        "    f1 = f1_score(all_labels, binary_preds)\n",
        "    precision = precision_score(all_labels, binary_preds)\n",
        "    recall = recall_score(all_labels, binary_preds)\n",
        "\n",
        "    avg_loss = total_loss / total_examples\n",
        "    return avg_loss, accuracy, f1, precision, recall\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = total_examples = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.train()\n",
        "    for sampled_data in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move `sampled_data` to the respective `device`\n",
        "        # Move sampled_data to the appropriate device\n",
        "        sampled_data = sampled_data.to(device)\n",
        "\n",
        "        #print(f\"Shape of input tensor : {sampled_data.x.shape}\")\n",
        "        # Run `forward` pass of the model\n",
        "        pred = model(sampled_data.x, sampled_data.edge_index, sampled_data.edge_label_index)\n",
        "\n",
        "        # Ground truth labels for link prediction (0 or 1)\n",
        "        ground_truth = sampled_data.edge_label\n",
        "        #print(f\"pred shape: {pred.shape}, ground_truth shape: {ground_truth.shape}\")\n",
        "\n",
        "        # Apply binary cross entropy with logits\n",
        "        loss = criterion(pred, ground_truth.float())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss) * pred.numel()\n",
        "        total_examples += pred.numel()\n",
        "\n",
        "        # Collect predictions and labels for metrics\n",
        "        all_preds.append(torch.sigmoid(pred).detach().cpu().numpy())  # Apply sigmoid to get probabilities\n",
        "        all_labels.append(ground_truth.cpu().numpy())\n",
        "\n",
        "    # Concatenate all predictions and labels for the epoch\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Convert probabilities to binary predictions using threshold 0.5\n",
        "    binary_preds = (all_preds >= 0.5).astype(int)\n",
        "\n",
        "    # Calculate training metrics\n",
        "    train_loss = total_loss / total_examples\n",
        "    train_accuracy = accuracy_score(all_labels, binary_preds)\n",
        "    train_f1 = f1_score(all_labels, binary_preds)\n",
        "    train_precision = precision_score(all_labels, binary_preds)\n",
        "    train_recall = recall_score(all_labels, binary_preds)\n",
        "\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {train_loss:.4f}, \"\n",
        "          f\"Accuracy: {train_accuracy:.4f}, F1: {train_f1:.4f}, \"\n",
        "          f\"Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
        "\n",
        "    # Save training metrics to the file\n",
        "    with open(\"/content/drive/MyDrive/GNN-Results/biogrid-human/DeepWalk/GraphMamba/training_metrics_fuzzy.txt\", \"a\") as file:\n",
        "        file.write(f\"{epoch:03d}\\t{train_loss:.4f}\\t{train_accuracy:.4f}\\t\"\n",
        "                   f\"{train_f1:.4f}\\t{train_precision:.4f}\\t{train_recall:.4f}\\n\")\n",
        "\n",
        "    # Evaluation at the end of each epoch\n",
        "    eval_loss, eval_accuracy, eval_f1, eval_precision, eval_recall = evaluate(test_loader, model)\n",
        "    print(f\"Evaluation - Loss: {eval_loss:.4f}, Accuracy: {eval_accuracy:.4f}, \"\n",
        "          f\"F1: {eval_f1:.4f}, Precision: {eval_precision:.4f}, Recall: {eval_recall:.4f}\\n\")\n",
        "\n",
        "    # Save evaluation metrics to the file\n",
        "    with open(\"/content/drive/MyDrive/GNN-Results/biogrid-human/DeepWalk/GraphMamba/evaluation_metrics_fuzzy.txt\", \"a\") as file:\n",
        "        file.write(f\"Eval\\t{eval_loss:.4f}\\t{eval_accuracy:.4f}\\t\"\n",
        "                   f\"{eval_f1:.4f}\\t{eval_precision:.4f}\\t{eval_recall:.4f}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YHDgeG5r1BVY"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
