{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9404"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = r\"C:\\Users\\vansh\\AI and ML reading material\\GraphGAN_Project\\GraphGAN\\bio-grid-human\\bio-grid-human_train.txt\"\n",
    "df = pd.read_csv(dataset,\n",
    "                sep = '\\t',\n",
    "                names = [\"NodeIDfrom\", \"NodeIDto\"],\n",
    "                )\n",
    "#create the graph networkx object from the above dataframe\n",
    "\n",
    "G = nx.from_pandas_edgelist(df = df,\n",
    "                             source = \"NodeIDfrom\",\n",
    "                             target = \"NodeIDto\",\n",
    "                             create_using=nx.Graph())\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.350393  ,  0.27482653, -0.04657786,  0.34706479,  0.21028991,\n",
       "         0.56745905,  0.21026784,  0.68328494, -0.66025382, -0.96301746,\n",
       "        -0.40459678, -0.10537465,  0.4645738 , -0.20851436, -0.68482751,\n",
       "         0.19438629, -0.16578251,  0.48305196, -0.51618749,  0.05385859,\n",
       "        -0.60018659, -0.2697354 , -0.09805907,  0.55923522,  0.49758846,\n",
       "         0.3781592 ,  0.36981776,  0.26204705,  0.33005163,  0.32088688,\n",
       "         0.42399544,  0.38767719, -0.31314573,  0.30350986, -0.79478496,\n",
       "        -0.25088778,  0.47500992, -0.05470566, -0.69956172,  0.37452087,\n",
       "         0.07660769, -0.23233452, -0.05459533, -0.27948675,  1.32820129,\n",
       "         0.04532199,  0.262227  , -0.01381476,  0.42898911,  0.42193207]),\n",
       " (9436, 50))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_embeddings(filename, n_node, n_embed):\n",
    "    \"\"\"read pretrained node embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()[1:]  # skip the first line\n",
    "        rng = np.random.default_rng(seed=42)\n",
    "        embedding_matrix = rng.standard_normal(size = (n_node, n_embed))\n",
    "        for line in lines:\n",
    "            emd = line.split()\n",
    "            embedding_matrix[int(emd[0]), :] = str_list_to_float(emd[1:])\n",
    "        return embedding_matrix\n",
    "\n",
    "def str_list_to_float(str_list):\n",
    "    return [float(item) for item in str_list]\n",
    "filename = r\"C:\\Users\\vansh\\AI and ML reading material\\GraphGAN_Project\\GraphGAN\\test_embeddings.emb\"\n",
    "X = read_embeddings(filename=filename,\n",
    "                                     n_node = 9436,\n",
    "                                     n_embed = 50)\n",
    "X[4095], X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed = 42)\n",
    "\n",
    "k=3\n",
    "X = np.array(X)\n",
    "node_embed_dim = 50\n",
    "centroid = rng.standard_normal(size = (k, node_embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#K-Means Algorithm for node embeddings\n",
    "#the embediddings are 1xd dimensional.\n",
    "class KMeansNodeClustering:\n",
    "\n",
    "  def __init__(self, X, k, node_embed_dim):\n",
    "      self.k = k #k is the number of cluster centres\n",
    "      self.X = X #dataset consisting of node embeddings\n",
    "      self.node_embed_dim = node_embed_dim #dimensions of node embeddings.\n",
    "      self.centroid = None #these will be the centres of our distribution\n",
    "\n",
    "  @staticmethod\n",
    "  def euclidean_measure(centroid, node_embed):  # calculates the distance of the k-dimensional node from the centre\n",
    "      return np.sqrt(np.sum((centroid - node_embed)**2, axis=1))\n",
    "\n",
    "  def dimensional_mean(cluster_num, node_embed_dim, cluster_indices, cluster_centres): #calculates the mean of of the arrays, dimension-wise\n",
    "        axis_centre = np.zeros(node_embed_dim)\n",
    "        for i in range(node_embed_dim):\n",
    "            y = 0\n",
    "            tup = cluster_indices[cluster_num].shape\n",
    "            shape = tup[0]\n",
    "            for x in range(shape):\n",
    "                y += X[cluster_indices[cluster_num][x]][0][i]\n",
    "            mean = np.mean(y)\n",
    "            axis_centre[i] += mean\n",
    "        axis_centre = axis_centre/np.linalg.norm(axis_centre)\n",
    "        return cluster_centres.append(axis_centre)\n",
    "\n",
    "  def fit(self, max_iterations = 200):\n",
    "      rng = np.random.default_rng(seed = 69)\n",
    "      self.centroid = rng.standard_normal(size = (self.k, self.node_embed_dim))\n",
    "      self.centroid  = (self.centroid)/(np.max(self.centroid))\n",
    "      for _ in range(max_iterations):\n",
    "          y = []\n",
    "\n",
    "          for node_embed in self.X:\n",
    "                distance = KMeansNodeClustering.euclidean_measure(node_embed = np.array(node_embed),\n",
    "                                                                  centroid = centroid)\n",
    "                cluster_num = np.argmin(distance)\n",
    "                y.append(cluster_num)\n",
    "\n",
    "          y = np.array(y) #stores the clustur number each of the nodes belong to\n",
    "\n",
    "          cluster_indices = [] #to know which node belongs to which cluster\n",
    "\n",
    "          for i in range(self.k):\n",
    "                cluster_indices.append(np.argwhere(y == i)) #returns every node which belongs to the same cluster\n",
    "          cluster_indices = np.array(cluster_indices, dtype = object)\n",
    "          cluster_centres = [] #stores the centres of the clusters\n",
    "\n",
    "          for j, indices in enumerate(cluster_indices): #cluster_indices contains the cluster numbers and the indices that belong to a particular cluster\n",
    "                #i = cluster number\n",
    "                #indices = indices of the nodes that belong to i.\n",
    "                if len(indices) == 0:\n",
    "                    cluster_centres.append(self.centroid[i])\n",
    "                else:\n",
    "                    KMeansNodeClustering.dimensional_mean(\n",
    "                                                          cluster_num = j,\n",
    "                                                          node_embed_dim=node_embed_dim,\n",
    "                                                          cluster_indices = cluster_indices,\n",
    "                                                          cluster_centres=cluster_centres)\n",
    "\n",
    "          self.centroid = np.array(cluster_centres)\n",
    "          y = np.array(y)\n",
    "      return y, cluster_indices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_nodes = KMeansNodeClustering(X = X,\n",
    "                                       k = k,\n",
    "                                       node_embed_dim = node_embed_dim)\n",
    "#cluster_indices_id stores the number_id of the clusters each embeddings is related to\n",
    "#cluster_indices stores all the nodes that belong to one particular cluster, for all the clusters.\n",
    "cluster_indices_id, cluster_indices = clustered_nodes.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02405422, -0.16886884,  0.0432893 , -0.02474142, -0.01689123,\n",
       "         0.02418717, -0.00320072,  0.31513042, -0.06785535, -0.02390776,\n",
       "        -0.01088693, -0.12602707,  0.09837792,  0.25126294, -0.14625928,\n",
       "         0.18393429,  0.0771853 ,  0.0515889 , -0.39699451, -0.01127475,\n",
       "        -0.06527127,  0.0611539 ,  0.08479745, -0.08870684, -0.03389312,\n",
       "         0.10036672, -0.0599632 ,  0.12631391, -0.09282915,  0.02503458,\n",
       "         0.24897943,  0.01432958, -0.29952319,  0.05440489,  0.02150592,\n",
       "         0.10075214,  0.19389914, -0.15815777, -0.17780445, -0.10890294,\n",
       "         0.19445924, -0.0771647 ,  0.01710894,  0.12998771,  0.29910963,\n",
       "        -0.0868231 ,  0.20431098, -0.08287313, -0.06864406,  0.08207669],\n",
       "       [ 0.01025421,  0.34934564, -0.04813136, -0.17420928,  0.13873117,\n",
       "        -0.00400628,  0.08470334,  0.41181313, -0.03107169,  0.11612725,\n",
       "        -0.11671086, -0.0533821 , -0.11453404, -0.09141086, -0.04753912,\n",
       "         0.06836953,  0.02549791,  0.17110002, -0.38285581,  0.00438771,\n",
       "        -0.26107371, -0.13723295, -0.16182949, -0.14853004, -0.1018812 ,\n",
       "        -0.10383901, -0.04820009,  0.07244287, -0.10950124,  0.05246948,\n",
       "         0.24079217, -0.06732905, -0.29526476,  0.00348021, -0.05029331,\n",
       "         0.0503976 , -0.00647576,  0.01560601,  0.00382968,  0.05304599,\n",
       "         0.04381903, -0.01117015, -0.04161598, -0.01380125,  0.0995763 ,\n",
       "        -0.07651459,  0.12716483, -0.19098461,  0.02610656, -0.06026028],\n",
       "       [ 0.002245  ,  0.09870027,  0.00083989, -0.02121944, -0.05588957,\n",
       "        -0.13149092,  0.16847097, -0.0161579 ,  0.0290466 , -0.25856086,\n",
       "        -0.05114284, -0.08784231,  0.01022714, -0.06205544, -0.23366263,\n",
       "         0.02389383, -0.00112222,  0.14883865,  0.05394902, -0.06373737,\n",
       "        -0.11231249,  0.35239579,  0.14557515,  0.08522604,  0.02434526,\n",
       "         0.23703062,  0.24760362, -0.20219087, -0.14884513, -0.03524833,\n",
       "        -0.0413847 , -0.12240013,  0.09774293, -0.16259667, -0.40341906,\n",
       "        -0.073914  ,  0.03780042,  0.01771613,  0.3105492 ,  0.06601277,\n",
       "        -0.05491924, -0.03722591,  0.02158802,  0.0452873 ,  0.22669368,\n",
       "         0.02240032,  0.04859114,  0.04636172,  0.20022253,  0.00138244]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_array = clustered_nodes.centroid #contains the centroids of all the clusteres.\n",
    "centroid_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\AppData\\Local\\Temp\\ipykernel_39776\\1584236110.py:2: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  cluster_indices_array = np.array([torch.tensor(np.squeeze(cluster_indices[0], axis = 1)),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([tensor([   0,    1,    3,  ..., 9427, 9429, 9432]),\n",
       "       tensor([   2,    9,   12,  ..., 9433, 9434, 9435]),\n",
       "       tensor([  30,   47,   73,  105,  132,  264,  306,  350,  401,  425,  448,  474,\n",
       "                552,  707,  709,  740, 1000, 1027, 1049, 1062, 1108, 1183, 1202, 1203,\n",
       "               1205, 1460, 1559, 1561, 1605, 1606, 1642, 1663, 1678, 1723, 1733, 1739,\n",
       "               1740, 1754, 1876, 2155, 2158, 2177, 2210, 2221, 2237, 2254, 2298, 2300,\n",
       "               2316, 2347, 2686, 2719, 2739, 2769, 2811, 2907, 2928, 2978, 2996, 3011,\n",
       "               3138, 3263, 3346, 3369, 3464, 3476, 3480, 3481, 3719, 3809, 3842, 3863,\n",
       "               3872, 3887, 3940, 3946, 4043, 4049, 4221, 4312, 4327, 4453, 4479, 4538,\n",
       "               4603, 4707, 4719, 4730, 4795, 4805, 4819, 4855, 5008, 5030, 5137, 5258,\n",
       "               5271, 5418, 5526, 5774, 5874, 5887, 5908, 5957, 6174, 6175, 6183, 6248,\n",
       "               6549, 6608, 6630, 6758, 6771, 6851, 6908, 6998, 7134, 7243, 7251, 7516,\n",
       "               7677, 7678, 7706, 7756, 7842, 7860, 7861, 7947, 7953, 7991, 8478, 8780])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_indices = np.array(cluster_indices, dtype=object)\n",
    "cluster_indices_array = np.array([torch.tensor(np.squeeze(cluster_indices[0], axis = 1)),\n",
    "                                  torch.tensor(np.squeeze(cluster_indices[1], axis = 1)),\n",
    "                                  torch.tensor(np.squeeze(cluster_indices[2], axis = 1)),\n",
    "                                  ], dtype = object)\n",
    "\n",
    "\n",
    "cluster_indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FuzzyLayer:\n",
    "    \"\"\"\n",
    "    This is the parent class which will contain variables for Antecedant, Inference and Consequent classes.\n",
    "\n",
    "    Args:\n",
    "        X: The dataset which contains all the embeddings. shape(no_of_nodes, node_embed_dim)\n",
    "        centroid_array: contains all the centroids of all the clusters. shape(no_of_clusters, node_embed_dim)\n",
    "        cluster_indices_array: contains all the indices that belong to one particular cluster. shape(no_of_clusters, *number_of_indices_per_cluster*)\n",
    "                                                                                               number_of_indices_per_cluster varies therefore it has no particular shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, centroid_array, cluster_indices_array):\n",
    "        self.X = torch.tensor(X, device=device, dtype=torch.float64)\n",
    "        self.centroid_array = torch.tensor(centroid_array, device=device, dtype=torch.float64)\n",
    "        self.cluster_indices_array = cluster_indices_array\n",
    "\n",
    "class Antecedant(FuzzyLayer):\n",
    "    \"\"\"\n",
    "    Antecedant part of the fuzzy logic system, it gives us membership functions for the node embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, centroid_array, cluster_indices_array):\n",
    "        super().__init__(X, centroid_array, cluster_indices_array)\n",
    "\n",
    "    def dot_product(self):\n",
    "        # Use matrix multiplication instead of nested loops for efficiency\n",
    "        dot_products = torch.matmul(self.X, self.centroid_array.T)\n",
    "        return dot_products\n",
    "\n",
    "    def get_points(self, cluster_num, embed_axis):\n",
    "        # Fetch points corresponding to the cluster_num and embed_axis\n",
    "        cluster_ids = torch.tensor(self.cluster_indices_array[cluster_num], device=device)\n",
    "        points = self.X[cluster_ids, embed_axis]\n",
    "        return points\n",
    "\n",
    "    def get_mean(self, cluster_num, embed_axis):\n",
    "        # Directly access the mean from centroid array\n",
    "        return self.centroid_array[cluster_num, embed_axis]\n",
    "\n",
    "    def get_standard_deviation(self, cluster_num, embed_axis):\n",
    "        # Compute the variance in a vectorized manner\n",
    "        points = self.get_points(cluster_num, embed_axis)\n",
    "        mean = self.get_mean(cluster_num, embed_axis)\n",
    "        variance = torch.var(points)\n",
    "        variance = torch.sqrt(variance)\n",
    "        return variance\n",
    "\n",
    "    def get_stddev_tensor(self, embed_axis):\n",
    "        # Vectorize variance computation for all clusters\n",
    "        stddev = torch.tensor([self.get_standard_deviation(cluster_num, embed_axis) for cluster_num in range(self.centroid_array.shape[0])], device=device)\n",
    "        return stddev\n",
    "\n",
    "    def gaussianMF(self, cluster_num, embed_axis, element):\n",
    "        # Vectorized Gaussian Membership Function calculation\n",
    "        mean = self.get_mean(cluster_num, embed_axis)\n",
    "        stddev_tensor = self.get_stddev_tensor(embed_axis)\n",
    "        if stddev_tensor[cluster_num] != 0:\n",
    "            gaussian = (element - mean) ** 2 / (2 * (stddev_tensor[cluster_num] ** 2))\n",
    "        else:\n",
    "            return 0.0\n",
    "        return torch.exp(-gaussian)\n",
    "\n",
    "    def get_membership_array(self, embed_axis):\n",
    "        # Preallocate the membership array\n",
    "        membership_array = torch.zeros((self.X.shape[0], self.centroid_array.shape[0]), device=device)\n",
    "        for node_embed_num in range(self.X.shape[0]):\n",
    "            for i in range(self.centroid_array.shape[0]):\n",
    "                membership = self.gaussianMF(cluster_num=i, embed_axis=embed_axis, element=self.X[node_embed_num, embed_axis])\n",
    "                membership_array[node_embed_num, i] = membership\n",
    "        return membership_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Antecedant(X = X,\n",
    "               centroid_array = centroid_array,\n",
    "               cluster_indices_array = cluster_indices_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\AppData\\Local\\Temp\\ipykernel_39776\\4202830216.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cluster_ids = torch.tensor(self.cluster_indices_array[cluster_num], device=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[4.9446e-01, 4.0103e-01, 9.9384e-01],\n",
       "         [1.4409e-03, 2.5523e-04, 9.4634e-01],\n",
       "         [9.8327e-01, 9.8399e-01, 9.9991e-01],\n",
       "         ...,\n",
       "         [8.2893e-01, 8.0486e-01, 9.9863e-01],\n",
       "         [8.2538e-01, 8.0072e-01, 9.9860e-01],\n",
       "         [5.4269e-01, 4.8098e-01, 9.9529e-01]],\n",
       "\n",
       "        [[6.1268e-01, 8.5506e-01, 9.9726e-01],\n",
       "         [9.3263e-02, 1.7161e-02, 9.7155e-01],\n",
       "         [9.5033e-01, 9.8689e-01, 9.9997e-01],\n",
       "         ...,\n",
       "         [2.5932e-01, 4.5053e-01, 9.9039e-01],\n",
       "         [1.4871e-01, 2.7989e-01, 9.8571e-01],\n",
       "         [2.9414e-01, 4.9953e-01, 9.9144e-01]],\n",
       "\n",
       "        [[2.2588e-02, 3.4584e-02, 9.3537e-01],\n",
       "         [1.8767e-01, 2.0313e-01, 9.6994e-01],\n",
       "         [5.3075e-01, 5.0453e-01, 9.8777e-01],\n",
       "         ...,\n",
       "         [6.0163e-01, 7.8424e-01, 9.9322e-01],\n",
       "         [5.3058e-01, 7.2601e-01, 9.9136e-01],\n",
       "         [9.9890e-01, 9.7479e-01, 9.9983e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[6.8289e-05, 5.5139e-17, 8.4344e-01],\n",
       "         [2.7363e-01, 1.5365e-02, 9.7178e-01],\n",
       "         [1.9301e-01, 4.5698e-03, 9.6491e-01],\n",
       "         ...,\n",
       "         [9.8269e-01, 8.4874e-01, 9.9997e-01],\n",
       "         [8.6067e-01, 4.4744e-01, 9.9837e-01],\n",
       "         [9.9988e-01, 9.7093e-01, 9.9987e-01]],\n",
       "\n",
       "        [[9.4323e-01, 8.7739e-01, 9.9999e-01],\n",
       "         [9.1428e-01, 7.9097e-01, 9.9992e-01],\n",
       "         [9.6790e-01, 7.4934e-01, 9.9874e-01],\n",
       "         ...,\n",
       "         [9.9981e-01, 9.6739e-01, 9.9960e-01],\n",
       "         [9.6712e-01, 9.4568e-01, 1.0000e+00],\n",
       "         [9.9992e-01, 9.7084e-01, 9.9961e-01]],\n",
       "\n",
       "        [[1.6042e-05, 8.0344e-10, 7.7484e-01],\n",
       "         [8.9672e-08, 2.5394e-14, 6.8490e-01],\n",
       "         [5.6071e-01, 4.6337e-01, 9.8900e-01],\n",
       "         ...,\n",
       "         [9.2190e-01, 9.6433e-01, 9.9904e-01],\n",
       "         [9.3841e-01, 9.7979e-01, 9.9933e-01],\n",
       "         [8.8116e-01, 5.8287e-01, 9.9524e-01]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "membership_array = []\n",
    "for embed_axis in range(node_embed_dim):\n",
    "    mem_array_per_axis = b.get_membership_array(embed_axis = embed_axis)\n",
    "    membership_array.append(mem_array_per_axis)\n",
    "membership_array = torch.stack(membership_array)\n",
    "membership_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np  # Only if absolutely necessary\n",
    "\n",
    "class Consequent(FuzzyLayer):\n",
    "    \"\"\"\n",
    "    This class generates the crisp embeddings using the membership values generated by the Antecedant class.\n",
    "\n",
    "    Essentially, we fuzzified each of the embeddings for each node, clustered the nodes, found centroids,\n",
    "    and calculated membership for each embedding. Now, we use that membership to output crisp memberships.\n",
    "\n",
    "    To do that, we generate sets with variances and treat them as the consequent fuzzy set from which crisp output is generated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, centroid_array, cluster_indices_array):\n",
    "        super().__init__(X, centroid_array, cluster_indices_array)\n",
    "\n",
    "    def get_average_membership_array(self, membership_array):\n",
    "        \"\"\"\n",
    "        Calculates the average membership for each embedding dimension across all clusters.\n",
    "\n",
    "        Args:\n",
    "            membership_array (torch.Tensor): The membership values of shape (node_embed_dim, num_nodes, num_clusters).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The average membership values of shape (node_embed_dim, num_nodes).\n",
    "        \"\"\"\n",
    "        return torch.mean(membership_array, dim=2)  # shape = (node_embed_dim, num_nodes)\n",
    "\n",
    "    def get_embed_axis_mean(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean of each embedding axis.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The mean values for each embedding axis of shape (node_embed_dim,).\n",
    "        \"\"\"\n",
    "        a = self.X.transpose(dim0=1, dim1=0) #shape = (50, 5242)\n",
    "        return np.mean(a, axis = 1) #shape = (50,)\n",
    "\n",
    "    def get_embed_axis_variance(self):\n",
    "        \"\"\"\n",
    "        Calculates the variance of each embedding axis.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The variance values for each embedding axis of shape (node_embed_dim,).\n",
    "        \"\"\"\n",
    "        a = self.X.transpose(dim0=1, dim1=0)\n",
    "        mean = self.get_embed_axis_mean()\n",
    "        variance = np.zeros(shape = node_embed_dim)\n",
    "        for i in range(node_embed_dim):\n",
    "            x = np.sqrt(np.mean((a[i] - mean[i])**2))\n",
    "            variance[i] += x\n",
    "        return variance #shape = (50,)\n",
    "\n",
    "    def get_crisp_embeddings(self, average_membership_array):\n",
    "        \"\"\"\n",
    "        Generates the crisp embeddings using the average membership values.\n",
    "\n",
    "        Args:\n",
    "            average_membership_array (torch.Tensor): The average membership values of shape (node_embed_dim, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The crisp embeddings of shape (num_nodes, node_embed_dim).\n",
    "        \"\"\"\n",
    "        a = self.X.transpose(dim0=1, dim1=0)\n",
    "        mean = self.get_embed_axis_mean()\n",
    "        variance = self.get_embed_axis_variance()\n",
    "        new_X = np.zeros(shape = (node_embed_dim, self.X.shape[0]))\n",
    "        for i in range(node_embed_dim):\n",
    "            for j in range(self.X.shape[0]):\n",
    "                if average_membership_array[i][j] == 0:\n",
    "                    new_X[i][j] = a[i][j]\n",
    "                else:\n",
    "                    b = -1*np.log(average_membership_array[i][j])\n",
    "                    if(a[i][j] < 0):\n",
    "                        b = -1*np.sqrt(b)\n",
    "                    else:\n",
    "                        b = np.sqrt(b)\n",
    "                    b = b*variance[i] + mean[i]\n",
    "                    new_X[i][j] = b\n",
    "        new_X = torch.transpose(new_X, dim0=1, dim1=0)\n",
    "        new_X.shape\n",
    "        #new_X = new_X/np.linalg.norm(new_X, axis = 0)\n",
    "        return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consequent(X = X,\n",
    "               centroid_array=centroid_array,\n",
    "               cluster_indices_array = cluster_indices_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6298, 0.3160, 0.9891,  ..., 0.8775, 0.8749, 0.6730],\n",
       "        [0.8217, 0.3607, 0.9791,  ..., 0.5667, 0.4714, 0.5950],\n",
       "        [0.3308, 0.4536, 0.6744,  ..., 0.7930, 0.7493, 0.9912],\n",
       "        ...,\n",
       "        [0.2812, 0.4203, 0.3875,  ..., 0.9438, 0.7688, 0.9902],\n",
       "        [0.9402, 0.9017, 0.9053,  ..., 0.9889, 0.9709, 0.9901],\n",
       "        [0.2583, 0.2283, 0.6710,  ..., 0.9618, 0.9725, 0.8198]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_membership = c.get_average_membership_array(membership_array = membership_array)\n",
    "average_membership \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (axis=int, dtype=NoneType, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m crisp \u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_crisp_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43maverage_membership_array\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maverage_membership\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(crisp, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(norm\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[40], line 65\u001b[0m, in \u001b[0;36mConsequent.get_crisp_embeddings\u001b[1;34m(self, average_membership_array)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mGenerates the crisp embeddings using the average membership values.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    torch.Tensor: The crisp embeddings of shape (num_nodes, node_embed_dim).\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mtranspose(dim0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embed_axis_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embed_axis_variance()\n\u001b[0;32m     67\u001b[0m new_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape \u001b[38;5;241m=\u001b[39m (node_embed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[40], line 37\u001b[0m, in \u001b[0;36mConsequent.get_embed_axis_mean\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03mCalculates the mean of each embedding axis.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    torch.Tensor: The mean values for each embedding axis of shape (node_embed_dim,).\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mtranspose(dim0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#shape = (50, 5242)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\vansh\\anaconda3\\envs\\graphgan\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3462\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3460\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3465\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (axis=int, dtype=NoneType, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "crisp = c.get_crisp_embeddings(average_membership_array = average_membership)\n",
    "norm = np.linalg.norm(crisp, axis = 1)\n",
    "print(norm.shape)\n",
    "for i in range(crisp.shape[0]):\n",
    "    crisp[i] = crisp[i]\n",
    "crisp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8961, -0.8521, -0.4684,  ..., -1.2175,  0.8649,  0.0586],\n",
       "        [-0.8627, -1.0469, -0.7183,  ...,  0.5976,  1.0827, -1.0759],\n",
       "        [ 0.9045,  0.8788, -0.8165,  ..., -0.3847,  0.8627, -0.6265],\n",
       "        ...,\n",
       "        [ 0.0794,  1.2068, -0.3584,  ..., -0.3674, -0.1348, -0.4478],\n",
       "        [-0.2495,  0.7903, -0.3105,  ..., -0.1242, -0.1881, -0.4330],\n",
       "        [-0.1506,  0.9469, -0.2369,  ..., -0.6082, -0.0816, -0.6880]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crisp[torch.tensor(np.array(G.nodes()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embedding_filename = r\"C:\\Users\\vansh\\AI and ML reading material\\GraphGAN_Project\\Fuzzy Representation Learning\\abc.txt\"\n",
    "embeddings = crisp[torch.tensor(np.array(G.nodes()))]\n",
    "index = np.array(G.nodes()).reshape(-1, 1)\n",
    "embedding_matrix = np.hstack([index, embeddings])\n",
    "embedding_list = embedding_matrix.tolist()\n",
    "embedding_str = [str(int(emb[0])) + \" \" + \" \".join([str(x) for x in emb[1:]]) + \"\\n\"\n",
    "                  for emb in embedding_list]\n",
    "with open(embedding_filename, \"w+\") as f:\n",
    "    lines = [str(G.number_of_nodes()) + \"\\t\" + str(node_embed_dim) + \"\\n\"] + embedding_str\n",
    "    f.writelines(lines)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
