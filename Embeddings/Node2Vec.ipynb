{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Path to the dataset edges file\"\n",
    "embedding_filename = \"Path where you want to save the embedding file\"\n",
    "walk_length = 100\n",
    "num_walks = 80\n",
    "embed_size=50\n",
    "iter = 1\n",
    "window_size=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NodeIDfrom</th>\n",
       "      <th>NodeIDto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62358</th>\n",
       "      <td>9431</td>\n",
       "      <td>1153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62359</th>\n",
       "      <td>9432</td>\n",
       "      <td>4234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62360</th>\n",
       "      <td>9433</td>\n",
       "      <td>4234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62361</th>\n",
       "      <td>9434</td>\n",
       "      <td>4234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62362</th>\n",
       "      <td>9435</td>\n",
       "      <td>9353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62363 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NodeIDfrom  NodeIDto\n",
       "0               1         0\n",
       "1               2         3\n",
       "2               0        28\n",
       "3               3         2\n",
       "4               4         5\n",
       "...           ...       ...\n",
       "62358        9431      1153\n",
       "62359        9432      4234\n",
       "62360        9433      4234\n",
       "62361        9434      4234\n",
       "62362        9435      9353\n",
       "\n",
       "[62363 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset,\n",
    "                sep = '\\t',\n",
    "                names = [\"NodeIDfrom\", \"NodeIDto\"],\n",
    "                skiprows=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9436"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the graph networkx object from the above dataframe\n",
    "\n",
    "G = nx.from_pandas_edgelist(df = df,\n",
    "                             source = \"NodeIDfrom\",\n",
    "                             target = \"NodeIDto\",\n",
    "                             create_using=nx.Graph())\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_num(num, workers):\n",
    "    if num % workers == 0:\n",
    "        return [num // workers] * workers\n",
    "    else:\n",
    "        return [num // workers] * workers + [num % workers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "                                 (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random() * N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class RandomWalker:\n",
    "    def __init__(self, G, use_rejection_sampling=False):\n",
    "        \"\"\"\n",
    "        :param G: networkx Graph Object.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.use_rejection_sampling = use_rejection_sampling\n",
    "\n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = list(G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    walk.append(\n",
    "                        cur_nbrs[alias_sample(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    edge = (prev, cur)\n",
    "                    next_node = cur_nbrs[alias_sample(alias_edges[edge][0],\n",
    "                                                      alias_edges[edge][1])]\n",
    "                    walk.append(next_node)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n",
    "    \n",
    "    def simulate_walks(self, num_walks, walk_length, workers=1, verbose=0):\n",
    "\n",
    "        G = self.G\n",
    "\n",
    "        nodes = list(G.nodes())\n",
    "\n",
    "        results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
    "            delayed(self._simulate_walks)(nodes, num, walk_length) for num in\n",
    "            partition_num(num_walks, workers))\n",
    "\n",
    "        walks = list(itertools.chain(*results))\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def _simulate_walks(self, nodes, num_walks, walk_length, ):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                walks.append(self.node2vec_walk(\n",
    "                    walk_length=walk_length, start_node=v))\n",
    "        return walks\n",
    "    \n",
    "    def preprocess_transition_probs(self):\n",
    "        \"\"\"\n",
    "        Preprocessing of transition probabilities for guiding the random walks.\n",
    "        \"\"\"\n",
    "        G = self.G\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr].get('weight', 1.0)\n",
    "                                  for nbr in G.neighbors(node)]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs = [\n",
    "                float(u_prob) / norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = create_alias_table(normalized_probs)\n",
    "\n",
    "        if not self.use_rejection_sampling:\n",
    "            alias_edges = {}\n",
    "\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "                if not G.is_directed():\n",
    "                    alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "                self.alias_edges = alias_edges\n",
    "\n",
    "        self.alias_nodes = alias_nodes\n",
    "        return\n",
    "    \n",
    "    def get_alias_edge(self, t, v):\n",
    "        \"\"\"\n",
    "        compute unnormalized transition probability between nodes v and its neighbors give the previous visited node t.\n",
    "        :param t:\n",
    "        :param v:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        G = self.G\n",
    "        p = 1\n",
    "        q = 1\n",
    "\n",
    "        unnormalized_probs = []\n",
    "        for x in G.neighbors(v):\n",
    "            weight = G[v][x].get('weight', 1.0)  # w_vx\n",
    "            if x == t:  # d_tx == 0\n",
    "                unnormalized_probs.append(weight / p)\n",
    "            elif G.has_edge(x, t):  # d_tx == 1\n",
    "                unnormalized_probs.append(weight)\n",
    "            else:  # d_tx > 1\n",
    "                unnormalized_probs.append(weight / q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs = [\n",
    "            float(u_prob) / norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return create_alias_table(normalized_probs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "class Node2Vec:\n",
    "\n",
    "    def __init__(self, graph, walk_length, num_walks, workers=1, use_rejection_sampling = False):\n",
    "\n",
    "        self.graph = graph\n",
    "        self.walker = RandomWalker(G)\n",
    "\n",
    "        print(\"Preprocess transition probs...\")\n",
    "        self.walker.preprocess_transition_probs()\n",
    "\n",
    "        self.sentences = self.walker.simulate_walks(\n",
    "            num_walks=num_walks, walk_length=walk_length, workers=workers, verbose=1)\n",
    "\n",
    "    def train(self, embed_size=128, window_size=5, workers=3, iter=5, **kwargs):\n",
    "        kwargs[\"sentences\"] = self.sentences\n",
    "        kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "        kwargs[\"vector_size\"] = embed_size\n",
    "        kwargs[\"sg\"] = 1\n",
    "        kwargs[\"hs\"] = 0  # node2vec not use Hierarchical Softmax\n",
    "        kwargs[\"workers\"] = workers\n",
    "        kwargs[\"window\"] = window_size\n",
    "        kwargs[\"epochs\"] = iter\n",
    "\n",
    "        print(\"Learning embedding vectors...\")\n",
    "        model = Word2Vec(**kwargs)\n",
    "        print(\"Learning embedding vectors done!\")\n",
    "\n",
    "        self.w2v_model = model\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess transition probs...\n",
      "Learning embedding vectors...\n",
      "Learning embedding vectors done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x30098ff70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Node2Vec(graph = G,\n",
    "                 walk_length = 100,\n",
    "                 num_walks = 80)\n",
    "model.train(embed_size=50,\n",
    "            iter = 1,\n",
    "            window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11270191,  0.49906412,  0.50348854, -0.34926018,  0.2177425 ,\n",
       "       -0.12523465,  0.8685004 ,  0.3002656 ,  0.7236134 , -0.14671412,\n",
       "       -0.31611195, -0.11450509,  0.42640367,  0.15217224,  0.50814617,\n",
       "        0.04632494,  0.17245506, -0.33699244, -0.7271709 ,  0.61405146,\n",
       "       -0.01804817,  0.09432548, -0.33093464,  0.38034645, -0.10888965,\n",
       "        0.3004258 ,  0.27281588,  0.28487545,  0.00183553,  0.43650576,\n",
       "        0.49253207, -0.18696032,  0.0210338 , -0.09092064,  0.42407435,\n",
       "       -0.4031546 , -0.57301676,  0.05752803, -0.64204496, -0.4661547 ,\n",
       "       -0.82910365, -0.20246585,  0.78567225,  0.05741467,  0.20295218,\n",
       "        0.27117878,  0.11639062, -0.48698613,  0.64499944, -0.45387548],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w2v_model.wv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_filename = \"/Users/vanshgupta/Desktop/AI and ML reading material/GraphGAN_Project/Emb and Data/Emb/biogrid-human/Node2Vec/emb.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.4411929 , -0.23272078,  0.34105706, -0.12732305,  0.14284867,\n",
       "        -0.05249456,  1.3319893 , -0.07911512,  0.5341532 ,  0.18609825,\n",
       "        -0.0388088 ,  0.37535933,  0.07543252,  0.21904887, -0.22800668,\n",
       "        -0.60518056, -0.7063029 , -0.6923662 , -0.770631  ,  0.6574531 ,\n",
       "         0.2216889 , -0.4124251 ,  0.14412065,  0.05070386, -0.05478349,\n",
       "         0.06379271,  0.7106666 ,  0.20269437,  0.63162607,  0.1648631 ,\n",
       "         0.74775845, -0.9446173 , -0.45626616, -0.23624764,  0.64157426,\n",
       "         0.06679013, -0.621122  , -0.05883688, -0.0942089 , -0.5949024 ,\n",
       "        -0.63340694, -0.3138731 , -0.13964635,  0.10933653, -0.21421202,\n",
       "         0.07387857,  0.3730023 , -0.35212922,  0.32944885, -0.34283614],\n",
       "       dtype=float32),\n",
       " 3.5945706)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "for i in G.nodes():\n",
    "    embeddings.append(model.w2v_model.wv[i]) #/np.linalg.norm(model.wv[i]))\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings[0], np.max(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "index = np.array(G.nodes()).reshape(-1, 1)\n",
    "embedding_matrix = np.hstack([index, embeddings])\n",
    "embedding_list = embedding_matrix.tolist()\n",
    "embedding_str = [str(int(emb[0])) + \" \" + \" \".join([str(x) for x in emb[1:]]) + \"\\n\"\n",
    "                  for emb in embedding_list]\n",
    "with open(embedding_filename, \"w+\") as f:\n",
    "    lines = [str(G.number_of_nodes()) + \"\\t\" + str(50) + \"\\n\"] + embedding_str\n",
    "    f.writelines(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
